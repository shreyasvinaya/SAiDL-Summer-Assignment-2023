{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGayTSN1eDLD5Go+XiaXvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasvinaya/SAiDL-Summer-Assignment-2023/blob/main/SAIDL_Assignment_CORE_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_probability"
      ],
      "metadata": {
        "id": "EEUlmsxLZTmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vUzY-samVcA4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import tensorflow_probability as tfp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-100 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n"
      ],
      "metadata": {
        "id": "jUNlDnMCVhvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8eafdb-2872-4b03-fc12-5d0551c7024c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Softmax"
      ],
      "metadata": {
        "id": "0W7RiS1x31MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model with standard softmax\n",
        "model = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(100, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=30,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LvyRc7jXcUe",
        "outputId": "6ec4d632-ac35-469d-e8b4-2c044a14a753"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 5, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6400)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3277312   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               51300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,717,028\n",
            "Trainable params: 3,717,028\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 25s 8ms/step - loss: 4.0031 - accuracy: 0.0843 - val_loss: 3.5315 - val_accuracy: 0.1682\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 3.3048 - accuracy: 0.2036 - val_loss: 3.2049 - val_accuracy: 0.2284\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.9552 - accuracy: 0.2661 - val_loss: 2.9444 - val_accuracy: 0.2819\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 2.7363 - accuracy: 0.3115 - val_loss: 2.8302 - val_accuracy: 0.2992\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.5852 - accuracy: 0.3400 - val_loss: 2.6768 - val_accuracy: 0.3348\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.4611 - accuracy: 0.3655 - val_loss: 2.5835 - val_accuracy: 0.3538\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 2.3636 - accuracy: 0.3862 - val_loss: 2.6744 - val_accuracy: 0.3375\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.2672 - accuracy: 0.4057 - val_loss: 2.5245 - val_accuracy: 0.3758\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.1921 - accuracy: 0.4223 - val_loss: 2.5450 - val_accuracy: 0.3689\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.1201 - accuracy: 0.4371 - val_loss: 2.5692 - val_accuracy: 0.3710\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 2.0613 - accuracy: 0.4505 - val_loss: 2.4864 - val_accuracy: 0.3888\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.9954 - accuracy: 0.4641 - val_loss: 2.4358 - val_accuracy: 0.3937\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.9302 - accuracy: 0.4783 - val_loss: 2.4505 - val_accuracy: 0.3962\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8922 - accuracy: 0.4846 - val_loss: 2.4564 - val_accuracy: 0.3913\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8465 - accuracy: 0.4954 - val_loss: 2.3924 - val_accuracy: 0.4094\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8042 - accuracy: 0.5095 - val_loss: 2.4393 - val_accuracy: 0.4121\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.7578 - accuracy: 0.5166 - val_loss: 2.6216 - val_accuracy: 0.3810\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.7221 - accuracy: 0.5257 - val_loss: 2.4676 - val_accuracy: 0.4100\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6825 - accuracy: 0.5314 - val_loss: 2.5068 - val_accuracy: 0.3977\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6455 - accuracy: 0.5396 - val_loss: 2.5181 - val_accuracy: 0.4030\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6112 - accuracy: 0.5511 - val_loss: 2.4815 - val_accuracy: 0.4103\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5846 - accuracy: 0.5561 - val_loss: 2.5997 - val_accuracy: 0.3929\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5532 - accuracy: 0.5629 - val_loss: 2.4890 - val_accuracy: 0.4095\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.5305 - accuracy: 0.5691 - val_loss: 2.5048 - val_accuracy: 0.4111\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4917 - accuracy: 0.5776 - val_loss: 2.6082 - val_accuracy: 0.3941\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4772 - accuracy: 0.5810 - val_loss: 2.5164 - val_accuracy: 0.4126\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4394 - accuracy: 0.5923 - val_loss: 2.6826 - val_accuracy: 0.3964\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4206 - accuracy: 0.5975 - val_loss: 2.5838 - val_accuracy: 0.4090\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3929 - accuracy: 0.6024 - val_loss: 2.6550 - val_accuracy: 0.3960\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3746 - accuracy: 0.6067 - val_loss: 2.5671 - val_accuracy: 0.4171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkSAmmTAXcR4",
        "outputId": "d043f7ef-1ca4-4c6c-ffb9-485dadac725b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 2.5671 - accuracy: 0.4171\n",
            "Test accuracy: 0.4171000123023987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate other metrics (precision, recall, F1 score, confusion matrix) using test set predictions\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jUCXZYzGXcPS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(classification_report(true_classes, predicted_classes))\n",
        "print(confusion_matrix(true_classes, predicted_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIiFl4NNXcM7",
        "outputId": "88d918c8-0953-4ac5-d029-4511d7295c56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.68       100\n",
            "           1       0.58      0.53      0.55       100\n",
            "           2       0.27      0.35      0.31       100\n",
            "           3       0.25      0.19      0.22       100\n",
            "           4       0.12      0.42      0.19       100\n",
            "           5       0.37      0.34      0.35       100\n",
            "           6       0.46      0.47      0.46       100\n",
            "           7       0.52      0.45      0.48       100\n",
            "           8       0.45      0.53      0.49       100\n",
            "           9       0.68      0.50      0.57       100\n",
            "          10       0.39      0.22      0.28       100\n",
            "          11       0.24      0.21      0.22       100\n",
            "          12       0.45      0.41      0.43       100\n",
            "          13       0.39      0.35      0.37       100\n",
            "          14       0.56      0.34      0.42       100\n",
            "          15       0.35      0.24      0.29       100\n",
            "          16       0.61      0.35      0.45       100\n",
            "          17       0.62      0.49      0.55       100\n",
            "          18       0.64      0.29      0.40       100\n",
            "          19       0.42      0.25      0.31       100\n",
            "          20       0.64      0.74      0.69       100\n",
            "          21       0.44      0.50      0.47       100\n",
            "          22       0.36      0.33      0.35       100\n",
            "          23       0.63      0.51      0.56       100\n",
            "          24       0.74      0.64      0.69       100\n",
            "          25       0.45      0.25      0.32       100\n",
            "          26       0.36      0.38      0.37       100\n",
            "          27       0.19      0.32      0.24       100\n",
            "          28       0.69      0.56      0.62       100\n",
            "          29       0.60      0.43      0.50       100\n",
            "          30       0.41      0.38      0.40       100\n",
            "          31       0.30      0.42      0.35       100\n",
            "          32       0.38      0.36      0.37       100\n",
            "          33       0.32      0.37      0.34       100\n",
            "          34       0.32      0.34      0.33       100\n",
            "          35       0.31      0.18      0.23       100\n",
            "          36       0.35      0.33      0.34       100\n",
            "          37       0.33      0.37      0.35       100\n",
            "          38       0.22      0.13      0.16       100\n",
            "          39       0.52      0.50      0.51       100\n",
            "          40       0.53      0.38      0.44       100\n",
            "          41       0.66      0.62      0.64       100\n",
            "          42       0.25      0.54      0.35       100\n",
            "          43       0.34      0.43      0.38       100\n",
            "          44       0.20      0.19      0.19       100\n",
            "          45       0.25      0.39      0.31       100\n",
            "          46       0.27      0.22      0.24       100\n",
            "          47       0.34      0.54      0.42       100\n",
            "          48       0.62      0.72      0.66       100\n",
            "          49       0.50      0.64      0.56       100\n",
            "          50       0.22      0.22      0.22       100\n",
            "          51       0.37      0.31      0.34       100\n",
            "          52       0.42      0.72      0.53       100\n",
            "          53       0.68      0.70      0.69       100\n",
            "          54       0.54      0.56      0.55       100\n",
            "          55       0.13      0.08      0.10       100\n",
            "          56       0.52      0.56      0.54       100\n",
            "          57       0.60      0.38      0.47       100\n",
            "          58       0.71      0.48      0.57       100\n",
            "          59       0.32      0.37      0.34       100\n",
            "          60       0.73      0.83      0.78       100\n",
            "          61       0.47      0.54      0.50       100\n",
            "          62       0.60      0.44      0.51       100\n",
            "          63       0.33      0.44      0.37       100\n",
            "          64       0.21      0.11      0.14       100\n",
            "          65       0.31      0.24      0.27       100\n",
            "          66       0.39      0.43      0.41       100\n",
            "          67       0.36      0.26      0.30       100\n",
            "          68       0.82      0.75      0.78       100\n",
            "          69       0.64      0.58      0.61       100\n",
            "          70       0.56      0.53      0.55       100\n",
            "          71       0.58      0.64      0.61       100\n",
            "          72       0.20      0.14      0.16       100\n",
            "          73       0.43      0.36      0.39       100\n",
            "          74       0.21      0.26      0.23       100\n",
            "          75       0.72      0.58      0.64       100\n",
            "          76       0.64      0.70      0.67       100\n",
            "          77       0.27      0.24      0.25       100\n",
            "          78       0.12      0.18      0.14       100\n",
            "          79       0.41      0.43      0.42       100\n",
            "          80       0.20      0.28      0.23       100\n",
            "          81       0.45      0.40      0.43       100\n",
            "          82       0.73      0.72      0.72       100\n",
            "          83       0.45      0.35      0.39       100\n",
            "          84       0.37      0.28      0.32       100\n",
            "          85       0.42      0.62      0.50       100\n",
            "          86       0.61      0.47      0.53       100\n",
            "          87       0.49      0.48      0.49       100\n",
            "          88       0.38      0.47      0.42       100\n",
            "          89       0.29      0.63      0.40       100\n",
            "          90       0.32      0.35      0.33       100\n",
            "          91       0.50      0.51      0.50       100\n",
            "          92       0.36      0.14      0.20       100\n",
            "          93       0.26      0.16      0.20       100\n",
            "          94       0.60      0.80      0.68       100\n",
            "          95       0.65      0.42      0.51       100\n",
            "          96       0.29      0.29      0.29       100\n",
            "          97       0.38      0.41      0.39       100\n",
            "          98       0.24      0.12      0.16       100\n",
            "          99       0.51      0.44      0.47       100\n",
            "\n",
            "    accuracy                           0.42     10000\n",
            "   macro avg       0.44      0.42      0.42     10000\n",
            "weighted avg       0.44      0.42      0.42     10000\n",
            "\n",
            "[[67  4  1 ...  0  0  0]\n",
            " [ 0 53  0 ...  0  0  0]\n",
            " [ 0  0 35 ...  2  5  0]\n",
            " ...\n",
            " [ 0  0  0 ... 41  0  0]\n",
            " [ 3  1 11 ...  1 12  0]\n",
            " [ 0  0  1 ...  0  0 44]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F74DsUMYXcKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPeAFTYjXcH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmZ0yqWpXcFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gumbell Softmax"
      ],
      "metadata": {
        "id": "jEeu9LRH3xza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfd = tfp.distributions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AITCL1hUXcCi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom Gumbel-Softmax layer\n",
        "class GumbelSoftmaxLayer(layers.Layer):\n",
        "    def __init__(self, num_classes, temperature, **kwargs):\n",
        "        super(GumbelSoftmaxLayer, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def call(self, inputs):\n",
        "        gumbel_dist = tfd.RelaxedOneHotCategorical(self.temperature, logits=inputs)\n",
        "        return gumbel_dist.sample()\n"
      ],
      "metadata": {
        "id": "uCJuzXfFZ8AU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CNN model with Gumbel-Softmax\n",
        "model_gumbel = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(100),\n",
        "    GumbelSoftmaxLayer(100, temperature=0.5)\n",
        "])\n",
        "\n",
        "model_gumbel.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_gumbel = model_gumbel.fit(train_images, train_labels, epochs=30,\n",
        "                                  validation_data=(test_images, test_labels))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1NbvB-HqMhB",
        "outputId": "09b32919-8857-4f92-82a9-196392a96ad9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 18s 10ms/step - loss: 8.6229 - accuracy: 0.0335 - val_loss: 7.7243 - val_accuracy: 0.0573\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 7.3490 - accuracy: 0.0865 - val_loss: 6.9433 - val_accuracy: 0.0999\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 6.7285 - accuracy: 0.1281 - val_loss: 6.4126 - val_accuracy: 0.1412\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 6.2956 - accuracy: 0.1601 - val_loss: 6.2438 - val_accuracy: 0.1657\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 5.9940 - accuracy: 0.1871 - val_loss: 5.8395 - val_accuracy: 0.2035\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 5.7869 - accuracy: 0.2044 - val_loss: 6.0329 - val_accuracy: 0.2016\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 5.5838 - accuracy: 0.2232 - val_loss: 5.7917 - val_accuracy: 0.2181\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 5.3731 - accuracy: 0.2405 - val_loss: 5.5933 - val_accuracy: 0.2403\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 5.2753 - accuracy: 0.2540 - val_loss: 5.5361 - val_accuracy: 0.2506\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 5.1263 - accuracy: 0.2698 - val_loss: 5.6577 - val_accuracy: 0.2417\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.9658 - accuracy: 0.2841 - val_loss: 5.4773 - val_accuracy: 0.2603\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.9029 - accuracy: 0.2962 - val_loss: 5.5352 - val_accuracy: 0.2651\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 4.7893 - accuracy: 0.3065 - val_loss: 5.4177 - val_accuracy: 0.2707\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.6673 - accuracy: 0.3180 - val_loss: 5.4338 - val_accuracy: 0.2686\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.6139 - accuracy: 0.3248 - val_loss: 5.3524 - val_accuracy: 0.2815\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.5091 - accuracy: 0.3368 - val_loss: 5.3608 - val_accuracy: 0.2774\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.4179 - accuracy: 0.3489 - val_loss: 5.6419 - val_accuracy: 0.2764\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.3546 - accuracy: 0.3539 - val_loss: 5.5014 - val_accuracy: 0.2761\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.2373 - accuracy: 0.3685 - val_loss: 5.3626 - val_accuracy: 0.2955\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 4.1922 - accuracy: 0.3719 - val_loss: 5.4221 - val_accuracy: 0.2922\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.1346 - accuracy: 0.3817 - val_loss: 5.3709 - val_accuracy: 0.3062\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 4.0761 - accuracy: 0.3898 - val_loss: 5.5802 - val_accuracy: 0.2989\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.9672 - accuracy: 0.4033 - val_loss: 5.2551 - val_accuracy: 0.3157\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.9521 - accuracy: 0.4023 - val_loss: 5.4368 - val_accuracy: 0.3067\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.9077 - accuracy: 0.4111 - val_loss: 5.3845 - val_accuracy: 0.3162\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 3.8096 - accuracy: 0.4183 - val_loss: 5.3911 - val_accuracy: 0.3122\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.7917 - accuracy: 0.4230 - val_loss: 5.4739 - val_accuracy: 0.3147\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.7442 - accuracy: 0.4320 - val_loss: 5.4799 - val_accuracy: 0.3130\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.6776 - accuracy: 0.4382 - val_loss: 5.5534 - val_accuracy: 0.3172\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 3.6449 - accuracy: 0.4434 - val_loss: 5.5476 - val_accuracy: 0.3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss_gumbel, test_acc_gumbel = model_gumbel.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy (Gumbel-Softmax): {test_acc_gumbel}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vClslvN3qOgl",
        "outputId": "5d425312-c4ad-43e7-dbaf-bf37462da45f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 5.4930 - accuracy: 0.3241\n",
            "Test accuracy (Gumbel-Softmax): 0.32409998774528503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate other metrics (precision, recall, F1 score, confusion matrix) using test set predictions\n",
        "predictions_gumbel = model_gumbel.predict(test_images)\n",
        "predicted_classes_gumbel = np.argmax(predictions_gumbel, axis=1)\n",
        "\n",
        "print(classification_report(true_classes, predicted_classes_gumbel))\n",
        "print(confusion_matrix(true_classes, predicted_classes_gumbel))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H0bXltUqQh_",
        "outputId": "c0059bdb-46dc-4e0e-c7b7-a08b81644cf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62       100\n",
            "           1       0.42      0.48      0.45       100\n",
            "           2       0.23      0.19      0.21       100\n",
            "           3       0.12      0.08      0.10       100\n",
            "           4       0.10      0.17      0.12       100\n",
            "           5       0.36      0.21      0.26       100\n",
            "           6       0.28      0.46      0.35       100\n",
            "           7       0.36      0.40      0.38       100\n",
            "           8       0.30      0.37      0.33       100\n",
            "           9       0.63      0.40      0.49       100\n",
            "          10       0.19      0.14      0.16       100\n",
            "          11       0.22      0.11      0.15       100\n",
            "          12       0.31      0.28      0.30       100\n",
            "          13       0.34      0.30      0.32       100\n",
            "          14       0.29      0.23      0.26       100\n",
            "          15       0.26      0.20      0.23       100\n",
            "          16       0.42      0.31      0.36       100\n",
            "          17       0.59      0.39      0.47       100\n",
            "          18       0.12      0.11      0.12       100\n",
            "          19       0.23      0.16      0.19       100\n",
            "          20       0.66      0.66      0.66       100\n",
            "          21       0.34      0.53      0.42       100\n",
            "          22       0.29      0.32      0.30       100\n",
            "          23       0.43      0.38      0.40       100\n",
            "          24       0.64      0.50      0.56       100\n",
            "          25       0.30      0.20      0.24       100\n",
            "          26       0.23      0.30      0.26       100\n",
            "          27       0.17      0.42      0.24       100\n",
            "          28       0.63      0.51      0.56       100\n",
            "          29       0.31      0.30      0.30       100\n",
            "          30       0.28      0.29      0.29       100\n",
            "          31       0.27      0.30      0.28       100\n",
            "          32       0.23      0.23      0.23       100\n",
            "          33       0.34      0.35      0.35       100\n",
            "          34       0.25      0.20      0.22       100\n",
            "          35       0.18      0.16      0.17       100\n",
            "          36       0.23      0.25      0.24       100\n",
            "          37       0.26      0.17      0.21       100\n",
            "          38       0.10      0.12      0.11       100\n",
            "          39       0.34      0.48      0.40       100\n",
            "          40       0.37      0.26      0.31       100\n",
            "          41       0.66      0.55      0.60       100\n",
            "          42       0.17      0.37      0.24       100\n",
            "          43       0.23      0.24      0.24       100\n",
            "          44       0.10      0.17      0.13       100\n",
            "          45       0.14      0.15      0.15       100\n",
            "          46       0.31      0.15      0.20       100\n",
            "          47       0.43      0.38      0.40       100\n",
            "          48       0.49      0.62      0.55       100\n",
            "          49       0.38      0.44      0.41       100\n",
            "          50       0.12      0.12      0.12       100\n",
            "          51       0.25      0.31      0.28       100\n",
            "          52       0.38      0.78      0.51       100\n",
            "          53       0.59      0.47      0.52       100\n",
            "          54       0.40      0.45      0.42       100\n",
            "          55       0.06      0.08      0.07       100\n",
            "          56       0.57      0.39      0.46       100\n",
            "          57       0.46      0.27      0.34       100\n",
            "          58       0.50      0.35      0.41       100\n",
            "          59       0.33      0.26      0.29       100\n",
            "          60       0.73      0.69      0.71       100\n",
            "          61       0.41      0.47      0.44       100\n",
            "          62       0.44      0.34      0.38       100\n",
            "          63       0.22      0.34      0.27       100\n",
            "          64       0.09      0.07      0.08       100\n",
            "          65       0.18      0.15      0.16       100\n",
            "          66       0.23      0.29      0.26       100\n",
            "          67       0.12      0.06      0.08       100\n",
            "          68       0.73      0.71      0.72       100\n",
            "          69       0.66      0.49      0.56       100\n",
            "          70       0.29      0.19      0.23       100\n",
            "          71       0.46      0.42      0.44       100\n",
            "          72       0.09      0.06      0.07       100\n",
            "          73       0.25      0.18      0.21       100\n",
            "          74       0.22      0.27      0.24       100\n",
            "          75       0.57      0.57      0.57       100\n",
            "          76       0.71      0.48      0.57       100\n",
            "          77       0.12      0.10      0.11       100\n",
            "          78       0.14      0.17      0.15       100\n",
            "          79       0.26      0.21      0.23       100\n",
            "          80       0.09      0.06      0.07       100\n",
            "          81       0.28      0.40      0.33       100\n",
            "          82       0.58      0.66      0.62       100\n",
            "          83       0.33      0.21      0.26       100\n",
            "          84       0.27      0.25      0.26       100\n",
            "          85       0.24      0.56      0.33       100\n",
            "          86       0.51      0.33      0.40       100\n",
            "          87       0.65      0.40      0.49       100\n",
            "          88       0.20      0.25      0.22       100\n",
            "          89       0.29      0.45      0.36       100\n",
            "          90       0.29      0.24      0.26       100\n",
            "          91       0.55      0.44      0.49       100\n",
            "          92       0.25      0.23      0.24       100\n",
            "          93       0.19      0.17      0.18       100\n",
            "          94       0.58      0.60      0.59       100\n",
            "          95       0.45      0.31      0.37       100\n",
            "          96       0.20      0.25      0.22       100\n",
            "          97       0.19      0.29      0.23       100\n",
            "          98       0.12      0.09      0.10       100\n",
            "          99       0.37      0.29      0.33       100\n",
            "\n",
            "    accuracy                           0.32     10000\n",
            "   macro avg       0.33      0.32      0.32     10000\n",
            "weighted avg       0.33      0.32      0.32     10000\n",
            "\n",
            "[[58  3  1 ...  0  0  0]\n",
            " [ 0 48  0 ...  0  0  0]\n",
            " [ 0  0 19 ...  2  4  0]\n",
            " ...\n",
            " [ 0  0  1 ... 29  1  0]\n",
            " [ 0  1  4 ...  3  9  0]\n",
            " [ 1  1  0 ...  1  0 29]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUgiLojLuTQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0J_AlQ4uTOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Task"
      ],
      "metadata": {
        "id": "ONnOIIzp3rly"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n06KYBbguTL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4fdw0EXuTJ5",
        "outputId": "001f02a5-5e7b-4fe3-bfca-8e9bd6f57a45"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "# Load and preprocess the CIFAR-100 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the transformer-based architecture\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=100, ignore_mismatched_sizes=True)\n",
        "\n",
        "# Replace the standard softmax with alternative softmax function (e.g., Gumbel-Softmax)\n",
        "# Implement the alternative softmax function here\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Compile the model (optimizer, loss function, and evaluation metrics)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs.logits, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt66jRMCuTAr",
        "outputId": "042cd289-7400-48d1-8d8b-01f4eecc22f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5ZgKHPN1hEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}